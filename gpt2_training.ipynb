{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzpHvONH-2g3",
        "outputId": "8bac6df9-3ce5-4140-8aed-528c912e8024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install -Uq pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H51vMVNU5AF9",
        "outputId": "3607c4f6-0118-40f1-d6c0-bb43fda0329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvO9-EU2GXo0",
        "outputId": "425dad6a-97c8-4001-b8b4-241a12753fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TSAI-ERAv2-S21'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 30 (delta 19), reused 29 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (30/30), 769.15 KiB | 17.09 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Azreal18/TSAI-ERAv2-S21.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gHEo9KKoGczf"
      },
      "outputs": [],
      "source": [
        "## move python files to outside\n",
        "!mv TSAI-ERAv2-S21/*.py .\n",
        "!mv /content/TSAI-ERAv2-S21/input.txt ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VXdyZHDrsTy2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "from model import GPTConfig, GPT\n",
        "from data_loader import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C0WmV7i-JYm"
      },
      "source": [
        "## Training on tiny shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MYVVPDv_ESdr"
      },
      "outputs": [],
      "source": [
        "### hypre params\n",
        "max_lr = 6e-4\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "warmup_steps = 50\n",
        "max_steps = 1000\n",
        "\n",
        "# save / log config\n",
        "out_dir = 'saved_model'\n",
        "save_interval = 100\n",
        "log_interval = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-o8roesXFYw",
        "outputId": "e1f51630-da91-45b6-d73c-3ea121654068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# attempt to auto detect device\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "de97owKzCY6Q"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aUODjuWZbpk",
        "outputId": "efadea9f-9106-4b77-e52f-fb51c842d832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_batch_size = 524288, grad_accum_steps = 64\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = 524288 # to align with gpt2 training batch size in number of tokens\n",
        "B = 8\n",
        "T = 1024\n",
        "assert total_batch_size % (B*T) == 0, \"make sure total_batch_size is a multiple of B*T\"\n",
        "grad_accum_steps = total_batch_size // (B*T)\n",
        "print(f\"total_batch_size = {total_batch_size}, grad_accum_steps = {grad_accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BQsk3iWDmSV",
        "outputId": "a1d61139-42d4-498b-883f-bf6c350ec195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([8, 1024]), torch.Size([8, 1024]))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader = DataLoader(B = B, T = T)\n",
        "x, y = train_loader.next_batch()\n",
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "02icu_90QZkP"
      },
      "outputs": [],
      "source": [
        "torch.set_float32_matmul_precision('high')\n",
        "model_args = dict(vocab_size=50304)\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf)\n",
        "model.to(device)\n",
        "model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "w3yCl_CcQbPG"
      },
      "outputs": [],
      "source": [
        "min_lr = max_lr * 0.1\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(decay_ratio * math.pi))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9-CDKLz-Zl1",
        "outputId": "24b60421-1c67-4d1f-b108-21ba1f0e243a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "Number of non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "Using fused AdamW: True\n",
            "Step 0 -- Loss: 10.983610 -- Learning Rate: 1.2000e-05 -- DT: 82783.14ms -- Tokens/sec: 6333.27\n",
            "Step 50 -- Loss: 5.541756 -- Learning Rate: 6.0000e-04 -- DT: 29102.40ms -- Tokens/sec: 18015.28\n",
            "Step 100 -- Loss: 4.379844 -- Learning Rate: 5.9632e-04 -- DT: 29355.49ms -- Tokens/sec: 17859.97\n",
            "saving checkpoint to saved_model\n",
            "Step 150 -- Loss: 3.653660 -- Learning Rate: 5.8537e-04 -- DT: 29450.93ms -- Tokens/sec: 17802.09\n",
            "Step 200 -- Loss: 2.798484 -- Learning Rate: 5.6746e-04 -- DT: 29472.14ms -- Tokens/sec: 17789.28\n",
            "saving checkpoint to saved_model\n",
            "Step 250 -- Loss: 1.983019 -- Learning Rate: 5.4307e-04 -- DT: 29544.74ms -- Tokens/sec: 17745.56\n",
            "Step 300 -- Loss: 1.056258 -- Learning Rate: 5.1287e-04 -- DT: 29373.25ms -- Tokens/sec: 17849.16\n",
            "saving checkpoint to saved_model\n",
            "Step 350 -- Loss: 0.395353 -- Learning Rate: 4.7768e-04 -- DT: 29368.42ms -- Tokens/sec: 17852.10\n",
            "Step 397 -- Loss: 0.096994 -- Learning Rate: 4.4091e-04 -- DT: 29095.11ms -- Tokens/sec: 18019.80\n",
            "saving checkpoint to saved_model\n",
            "Stopping training as reached target loss\n"
          ]
        }
      ],
      "source": [
        "optimizer = model.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "best_loss = 1e9\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    loss_accum = 0.0\n",
        "    # determine and set learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True):  # enable mixed precision training\n",
        "            logits, loss = model(x, y)\n",
        "        loss = loss / grad_accum_steps  # loss normalizer\n",
        "        loss_accum += loss.detach()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # use torch.nn.utils.clip_grad_norm_\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = (t1 - t0)\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if step % log_interval == 0 or step == max_steps - 1 or loss_accum.item() < 0.099999:\n",
        "        print(\n",
        "            f\"Step {step} -- Loss: {loss_accum.item():.6f} -- Learning Rate: {lr:.4e} -- DT: {dt * 1000:.2f}ms -- Tokens/sec: {tokens_per_sec:.2f}\"\n",
        "        )\n",
        "\n",
        "    if step % save_interval == 0 or step == max_steps - 1 or loss_accum.item() < 0.099999:\n",
        "        if loss_accum.item() < best_loss:\n",
        "            best_loss = loss_accum.item()\n",
        "            if step > 0:\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': step,\n",
        "                    'best_loss': best_loss,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'Checkpoint.pt'))\n",
        "\n",
        "    if loss_accum.item() < 0.099999:\n",
        "        print(\"Stopping training as reached target loss\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbTOYj9t-sXW",
        "outputId": "b0f56b1a-9751-4502-bdd4-5d56d4057e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU:0\n",
            "process      12282 uses    11842.000 MB GPU memory\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "print(torch.cuda.list_gpu_processes())\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6v4rZ99HVOJ"
      },
      "source": [
        "## Sample Generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kePJ2JPiHW_3"
      },
      "outputs": [],
      "source": [
        "max_length = 30\n",
        "num_return_sequences = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5Caqp4YOJYQe"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype = torch.long) # (8,)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5,8)\n",
        "x = tokens.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "y6nTHCYVJf7q"
      },
      "outputs": [],
      "source": [
        "# generate\n",
        "x = model.generate(x, max_new_tokens=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN3LRtLAJwtK",
        "outputId": "47fd32c6-9afd-4708-dd28-cb73bbbc478a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hello, I'm a language model, more the\n",
            "Can case my Recall, and you, unhappy hisAR LA embrure to the\n",
            "This can\n",
            "> Hello, I'm a language model, tune\n",
            "For bombers; was not are therickorsrat waves hath tell, that will wrought maliciousvy-\n",
            "> Hello, I'm a language model, what:\n",
            "ANUMN Angelo of th speech, plain you, poor Lancaster.agged\n",
            "He vow is\n",
            "> Hello, I'm a language model, plain and\n",
            "Meer her this do you sit\n",
            "She should goodnesspt\n",
            "Will Caliban, younger my\n",
            "> Hello, I'm a language model, credit hear;own philosophy\n",
            "incial in the oscill?\n",
            "Sirown so Marself:\n",
            "Lie they not\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj5ZwEpdG6kl"
      },
      "source": [
        "## Upload to hugging face model hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aD7EfJiIQXlb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('to_upload', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "h76jWzsfHAvD"
      },
      "outputs": [],
      "source": [
        "!cp model.py to_upload\n",
        "!cp -r saved_model to_upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "1ecb060ce99a4cb28e8be20d9db2d13b",
            "ae11184b434849fa8954631b855e6a5b",
            "a9c6012fe83541689b0293ac051b8ec5",
            "7d22d69c9f48405aa9d9e9deda122419",
            "fd0181f6e38d43669f324380c9e20a5d",
            "c3bd58c7b9294983a51c5c613f31833f",
            "95b67de1295a4ffab60b3d0f0bdf2df9",
            "7c65b5056afc4520b19b3b6af344cda8",
            "63c1dc9bf1e3433ea30d1d8af7b3e12e",
            "63802c0440064afbbb37735822616724",
            "9a91c874020f49cf90781e40f9d246aa"
          ]
        },
        "id": "6OAn-nHCHGE9",
        "outputId": "440ae639-0240-4370-dbd9-4a869b54aa43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ecb060ce99a4cb28e8be20d9db2d13b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Checkpoint.pt:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Azreal18/GPT2/commit/40537acd918615238b015319c83a8e66b486df79', commit_message='Upload folder using huggingface_hub', commit_description='', oid='40537acd918615238b015319c83a8e66b486df79', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "api.upload_folder(\n",
        "    folder_path=\"./to_upload\",\n",
        "    repo_id=\"Azreal18/GPT2\",\n",
        "    repo_type=\"model\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ecb060ce99a4cb28e8be20d9db2d13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae11184b434849fa8954631b855e6a5b",
              "IPY_MODEL_a9c6012fe83541689b0293ac051b8ec5",
              "IPY_MODEL_7d22d69c9f48405aa9d9e9deda122419"
            ],
            "layout": "IPY_MODEL_fd0181f6e38d43669f324380c9e20a5d"
          }
        },
        "63802c0440064afbbb37735822616724": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c1dc9bf1e3433ea30d1d8af7b3e12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c65b5056afc4520b19b3b6af344cda8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d22d69c9f48405aa9d9e9deda122419": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63802c0440064afbbb37735822616724",
            "placeholder": "​",
            "style": "IPY_MODEL_9a91c874020f49cf90781e40f9d246aa",
            "value": " 1.54G/1.54G [01:22&lt;00:00, 28.0MB/s]"
          }
        },
        "95b67de1295a4ffab60b3d0f0bdf2df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a91c874020f49cf90781e40f9d246aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c6012fe83541689b0293ac051b8ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c65b5056afc4520b19b3b6af344cda8",
            "max": 1544231130,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63c1dc9bf1e3433ea30d1d8af7b3e12e",
            "value": 1544231130
          }
        },
        "ae11184b434849fa8954631b855e6a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3bd58c7b9294983a51c5c613f31833f",
            "placeholder": "​",
            "style": "IPY_MODEL_95b67de1295a4ffab60b3d0f0bdf2df9",
            "value": "Checkpoint.pt: 100%"
          }
        },
        "c3bd58c7b9294983a51c5c613f31833f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0181f6e38d43669f324380c9e20a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
